{
	"name": "Boston house price prediction",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "testsparkpool01",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "63c7b476-fdf2-4501-bfc9-e81c895cede3"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/051ddeca-1ed6-4d8b-ba6f-1ff561e5f3b3/resourceGroups/bigdataqa/providers/Microsoft.Synapse/workspaces/yanjuntestws/bigDataPools/testsparkpool01",
				"name": "testsparkpool01",
				"type": "Spark",
				"endpoint": "https://yanjuntestws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/testsparkpool01",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Boston house price prediction with Vowpal Wabbit, LightGBM and Spark MLlib\n",
					"\n",
					"This notebook shows how to build simple regression models by using \n",
					"[Vowpal Wabbit (VW)](https://github.com/VowpalWabbit/vowpal_wabbit) and \n",
					"[LightGBM](https://github.com/microsoft/LightGBM) with MMLSpark.\n",
					" We also compare the results with \n",
					" [Spark MLlib Linear Regression](https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression)."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import math\n",
					"from matplotlib.colors import ListedColormap, Normalize\n",
					"from matplotlib.cm import get_cmap\n",
					"import matplotlib.pyplot as plt\n",
					"from mmlspark.train import ComputeModelStatistics\n",
					"from mmlspark.vw import VowpalWabbitRegressor, VowpalWabbitFeaturizer\n",
					"from mmlspark.lightgbm import LightGBMRegressor\n",
					"import numpy as np\n",
					"import pandas as pd\n",
					"from pyspark.ml.feature import VectorAssembler\n",
					"from pyspark.ml.regression import LinearRegression\n",
					"from sklearn.datasets import load_boston"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Prepare Dataset\n",
					"We use [*Boston house price* dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) \n",
					". \n",
					"The data was collected in 1978 from Boston area and consists of 506 entries with 14 features including the value of homes. \n",
					"We use `sklearn.datasets` module to download it easily, then split the set into training and testing by 75/25."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"boston = load_boston()\n",
					"\n",
					"feature_cols = ['f' + str(i) for i in range(boston.data.shape[1])]\n",
					"header = ['target'] + feature_cols\n",
					"df = spark.createDataFrame(\n",
					"    pd.DataFrame(data=np.column_stack((boston.target, boston.data)), columns=header)\n",
					").repartition(1)\n",
					"print(\"Dataframe has {} rows\".format(df.count()))\n",
					"display(df.limit(10).toPandas())"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"train_data, test_data = df.randomSplit([0.75, 0.25], seed=42)\n",
					"train_data.cache()\n",
					"test_data.cache()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"Following is the summary of the training set."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"display(train_data.summary().toPandas())"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"Plot feature distributions over different target values (house prices in our case)."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"features = train_data.columns[1:]\n",
					"values = train_data.drop('target').toPandas()\n",
					"ncols = 5\n",
					"nrows = math.ceil(len(features) / ncols)\n",
					"\n",
					"yy = [r['target'] for r in train_data.select('target').collect()]\n",
					"\n",
					"f, axes = plt.subplots(nrows, ncols, sharey=True, figsize=(30,10))\n",
					"f.tight_layout()\n",
					"\n",
					"for irow in range(nrows):\n",
					"    axes[irow][0].set_ylabel('target')\n",
					"    for icol in range(ncols):\n",
					"        try:\n",
					"            feat = features[irow*ncols + icol]\n",
					"            xx = values[feat]\n",
					"\n",
					"            axes[irow][icol].scatter(xx, yy, s=10, alpha=0.25)\n",
					"            axes[irow][icol].set_xlabel(feat)\n",
					"            axes[irow][icol].get_yaxis().set_ticks([])\n",
					"        except IndexError:\n",
					"            f.delaxes(axes[irow][icol])"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Baseline - Spark MLlib Linear Regressor\n",
					"\n",
					"First, we set a baseline performance by using Linear Regressor in Spark MLlib."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"featurizer = VectorAssembler(\n",
					"    inputCols=feature_cols,\n",
					"    outputCol='features'\n",
					")\n",
					"lr_train_data = featurizer.transform(train_data)['target', 'features']\n",
					"lr_test_data = featurizer.transform(test_data)['target', 'features']\n",
					"display(lr_train_data.limit(10).toPandas())"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# By default, `maxIter` is 100. Other params you may want to change include: `regParam`, `elasticNetParam`, etc.\n",
					"lr = LinearRegression(\n",
					"    labelCol='target',\n",
					")\n",
					"\n",
					"lr_model = lr.fit(lr_train_data)\n",
					"lr_predictions = lr_model.transform(lr_test_data)\n",
					"\n",
					"display(lr_predictions.limit(10).toPandas())"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"We evaluate the prediction result by using `mmlspark.train.ComputeModelStatistics` which returns four metrics:\n",
					"* [MSE (Mean Squared Error)](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
					"* [RMSE (Root Mean Squared Error)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) = sqrt(MSE)\n",
					"* [R quared](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
					"* [MAE (Mean Absolute Error)](https://en.wikipedia.org/wiki/Mean_absolute_error)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"metrics = ComputeModelStatistics(\n",
					"    evaluationMetric='regression',\n",
					"    labelCol='target',\n",
					"    scoresCol='prediction'\n",
					").transform(lr_predictions)\n",
					"\n",
					"results = metrics.toPandas()\n",
					"results.insert(0, 'model', ['Spark MLlib - Linear Regression'])\n",
					"display(results)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Vowpal Wabbit"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Perform VW-style feature hashing. Many types (numbers, string, bool, map of string to (number, string)) are supported."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"vw_featurizer = VowpalWabbitFeaturizer(\n",
					"    inputCols=feature_cols,\n",
					"    outputCol='features',\n",
					")\n",
					"vw_train_data = vw_featurizer.transform(train_data)['target', 'features']\n",
					"vw_test_data = vw_featurizer.transform(test_data)['target', 'features']\n",
					"display(vw_train_data.limit(10).toPandas())"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"See [VW wiki](https://github.com/vowpalWabbit/vowpal_wabbit/wiki/Command-Line-Arguments) for command line arguments."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# Use the same number of iterations as Spark MLlib's Linear Regression (=100)\n",
					"args = \"--holdout_off --loss_function quantile -l 7 -q :: --power_t 0.3\"\n",
					"vwr = VowpalWabbitRegressor(\n",
					"    labelCol='target',\n",
					"    args=args,\n",
					"    numPasses=100,\n",
					")\n",
					"\n",
					"# To reduce number of partitions (which will effect performance), use `vw_train_data.repartition(1)`\n",
					"vw_train_data_2 = vw_train_data.repartition(1).cache()\n",
					"print(vw_train_data_2.count())\n",
					"vw_model = vwr.fit(vw_train_data_2.repartition(1))\n",
					"vw_predictions = vw_model.transform(vw_test_data)\n",
					"\n",
					"display(vw_predictions.limit(10).toPandas())"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"metrics = ComputeModelStatistics(\n",
					"    evaluationMetric='regression',\n",
					"    labelCol='target',\n",
					"    scoresCol='prediction'\n",
					").transform(vw_predictions)\n",
					"\n",
					"vw_result = metrics.toPandas()\n",
					"vw_result.insert(0, 'model', ['Vowpal Wabbit'])\n",
					"results = results.append(\n",
					"    vw_result,\n",
					"    ignore_index=True\n",
					")\n",
					"display(results)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"source": [
					"## LightGBM"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"lgr = LightGBMRegressor(\n",
					"    objective='quantile',\n",
					"    alpha=0.2,\n",
					"    learningRate=0.3,\n",
					"    numLeaves=31,\n",
					"    labelCol='target',\n",
					"    numIterations=100,\n",
					")\n",
					"\n",
					"# Using one partition since the training dataset is very small\n",
					"repartitioned_data = lr_train_data.repartition(1).cache()\n",
					"print(repartitioned_data.count())\n",
					"lg_model = lgr.fit(repartitioned_data)\n",
					"lg_predictions = lg_model.transform(lr_test_data)\n",
					"\n",
					"display(lg_predictions.limit(10).toPandas())"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"metrics = ComputeModelStatistics(\n",
					"    evaluationMetric='regression',\n",
					"    labelCol='target',\n",
					"    scoresCol='prediction'\n",
					").transform(lg_predictions)\n",
					"\n",
					"lg_result = metrics.toPandas()\n",
					"lg_result.insert(0, 'model', ['LightGBM'])\n",
					"results = results.append(\n",
					"    lg_result,\n",
					"    ignore_index=True\n",
					")\n",
					"display(results)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"source": [
					"Following figure shows the actual-vs.-prediction graphs of the results:\n",
					"\n",
					"<img width=\"1102\" alt=\"lr-vw-lg\" src=\"https://user-images.githubusercontent.com/42475935/64071975-4c3e9600-cc54-11e9-8b1f-9a1ee300f445.png\">"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"cmap = get_cmap('YlOrRd')\n",
					"\n",
					"target = np.array(test_data.select('target').collect()).flatten()\n",
					"model_preds = [\n",
					"    (\"Spark MLlib Linear Regression\", lr_predictions),\n",
					"    (\"Vowpal Wabbit\", vw_predictions),\n",
					"    (\"LightGBM\", lg_predictions)\n",
					"]"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"f, axes = plt.subplots(1, len(model_preds), sharey=True, figsize=(18, 6))\r\n",
					"f.tight_layout()\r\n",
					"\r\n",
					"for i, (model_name, preds) in enumerate(model_preds):\r\n",
					"    preds = np.array(preds.select('prediction').collect()).flatten()\r\n",
					"    err = np.absolute(preds - target)\r\n",
					"\r\n",
					"    norm = Normalize()\r\n",
					"    clrs = cmap(np.asarray(norm(err)))[:, :-1]\r\n",
					"    axes[i].scatter(preds, target, s=60, c=clrs, edgecolors='#888888', alpha=0.75)\r\n",
					"    axes[i].plot((0, 60), (0, 60), linestyle='--', color='#888888')\r\n",
					"    axes[i].set_xlabel('Predicted values')\r\n",
					"    if i ==0:\r\n",
					"        axes[i].set_ylabel('Actual values')\r\n",
					"    axes[i].set_title(model_name)\r\n",
					"plt.show()"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Clean up resources\r\n",
					"To ensure the Spark instance is shut down, end any connected sessions(notebooks). The pool shuts down when the **idle time** specified in the Apache Spark pool is reached. You can also select **stop session** from the status bar at the upper right of the notebook.\r\n",
					"\r\n",
					"![stopsession](https://adsnotebookrelease.blob.core.windows.net/adsnotebookrelease/adsnotebook/image/stopsession.png)"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Next steps\r\n",
					"\r\n",
					"* [Check out Synapse sample notebooks](https://github.com/Azure-Samples/Synapse/tree/main/MachineLearning) \r\n",
					"* [MMLSpark GitHub Repo](https://github.com/Azure/mmlspark)"
				]
			}
		]
	}
}