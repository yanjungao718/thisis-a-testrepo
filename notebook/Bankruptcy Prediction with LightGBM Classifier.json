{
	"name": "Bankruptcy Prediction with LightGBM Classifier",
	"properties": {
		"description": "544545",
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "04650cf6-000a-44e2-8bc0-f8cbe199df9c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Bankruptcy Prediction with LightGBM Classifier\r\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Introduction of LightGBM\n",
					"[LightGBM](https://github.com/Microsoft/LightGBM) is an open-source, distributed, high-performance gradient boosting framework with following advantages: \n",
					"-   Composability: LightGBM models can be incorporated into existing\n",
					"    SparkML Pipelines, and used for batch, streaming, and serving\n",
					"    workloads.\n",
					"-   Performance: LightGBM on Spark is 10-30% faster than SparkML on\n",
					"    the Higgs dataset, and achieves a 15% increase in AUC.  [Parallel\n",
					"    experiments](https://github.com/Microsoft/LightGBM/blob/master/docs/Experiments.rst#parallel-experiment)\n",
					"    have verified that LightGBM can achieve a linear speed-up by using\n",
					"    multiple machines for training in specific settings.\n",
					"-   Functionality: LightGBM offers a wide array of [tunable\n",
					"    parameters](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst),\n",
					"    that one can use to customize their decision tree system. LightGBM on\n",
					"    Spark also supports new types of problems such as quantile regression.\n",
					"-   Cross platformï¼šLightGBM on Spark is available on Spark (Scala) and PySpark (Python).\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"<img src=\"https://mmlspark.blob.core.windows.net/graphics/Documentation/bankruptcy image.png\" width=\"800\" style=\"float: center;\"/>\n",
					"\n",
					"In this example, we use LightGBM to build a classification model in order to predict bankruptcy."
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Read dataset\r\n",
					"\r\n",
					"Get a sample data of financial statements for 6819 companies, 220 represents bankrupted companies while 6599 firms are not bankrupted. "
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"dataset = spark.read.format(\"csv\")\\\n",
					"  .option(\"header\", True)\\\n",
					"  .load(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/company_bankruptcy_prediction_data.csv\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Exploratory data\r\n",
					"\r\n",
					"Look at the data and evaluate its suitability for use in a model."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(dataset.head(5))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"# print dataset size\r\n",
					"print(\"Total number of records: \" + str(dataset.count()))"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# convert features to double type\n",
					"from pyspark.sql.functions import col\n",
					"from pyspark.sql.types import DoubleType\n",
					"for colName in dataset.columns:\n",
					"  dataset = dataset.withColumn(colName, col(colName).cast(DoubleType()))\n",
					"print(\"Schema: \")\n",
					"dataset.printSchema()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Generation of testing and training data sets\r\n",
					"\r\n",
					"Simple split, 85% for training and 15% for testing the model. Playing with this ratio may result in different models.\r\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Split the dataset into train and test\n",
					"\n",
					"train, test = dataset.randomSplit([0.70, 0.30], seed=1)\n",
					"\n",
					"# Add featurizer to convert features to vector\n",
					"\n",
					"from pyspark.ml.feature import VectorAssembler\n",
					"feature_cols = dataset.columns[1:]\n",
					"featurizer = VectorAssembler(\n",
					"    inputCols=feature_cols,\n",
					"    outputCol='features'\n",
					")\n",
					"train_data = featurizer.transform(train)['Bankrupt?', 'features']\n",
					"test_data = featurizer.transform(test)['Bankrupt?', 'features']"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# check if the data is unbalanced\r\n",
					"train_data.groupBy(\"Bankrupt?\").count().show()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Train the model\r\n",
					"Train the Classifier model."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"from mmlspark.lightgbm import LightGBMClassifier\n",
					"\n",
					"model = LightGBMClassifier(objective=\"binary\", featuresCol=\"features\", labelCol=\"Bankrupt?\", isUnbalance=True)\n",
					"model = model.fit(train_data)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from mmlspark.lightgbm import LightGBMClassificationModel\n",
					"model.saveNativeModel(\"/lgbmcmodel\")\n",
					"model = LightGBMClassificationModel.loadNativeModelFromFile(\"/lgbmcmodel\")"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"print(model.getFeatureImportances())"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Model Performance Evaluation\r\n",
					"\r\n",
					"After training the model, we evaluate the performance of the model using the test set."
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"predictions = model.transform(test_data)\n",
					"#predictions.limit(10).toPandas()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"from mmlspark.train import ComputeModelStatistics\r\n",
					"\r\n",
					"# Compute model performance metrics\r\n",
					"metrics = ComputeModelStatistics(evaluationMetric=\"classification\", \r\n",
					"                                 labelCol=\"prediction\", \r\n",
					"                                 scoredLabelsCol=\"Bankrupt?\").transform(predictions)\r\n",
					"metrics.toPandas()"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Clean up resources\r\n",
					"To ensure the Spark instance is shut down, end any connected sessions(notebooks). The pool shuts down when the **idle time** specified in the Apache Spark pool is reached. You can also select **stop session** from the status bar at the upper right of the notebook.\r\n",
					"\r\n",
					"![stopsession](https://adsnotebookrelease.blob.core.windows.net/adsnotebookrelease/adsnotebook/image/stopsession.png)"
				],
				"attachments": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Next steps\r\n",
					"\r\n",
					"* [Check out Synapse sample notebooks](https://github.com/Azure-Samples/Synapse/tree/main/MachineLearning) \r\n",
					"* [MMLSpark GitHub Repo](https://github.com/Azure/mmlspark)"
				],
				"attachments": null
			}
		]
	}
}